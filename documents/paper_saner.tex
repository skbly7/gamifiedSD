\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage[pdftex]{graphicx}
\graphicspath{ {images/} }
\usepackage{tabularx}
\usepackage{placeins}
\usepackage{array}

  \usepackage[pdftex]{graphicx}

\usepackage{flushend}
\clubpenalty = 10000
\widowpenalty = 10000
\displaywidowpenalty = 10000

\newcounter{magicrownumbers}
\newcommand\rownumber{\stepcounter{magicrownumbers}\arabic{magicrownumbers}}

\begin{document}

% paper title
\title{\huge  Quantitative Analysis on Impact of Gamification over Code Review Process. }\vspace{-2ex}


% author names and affiliations
\vspace{-2ex}\author{\IEEEauthorblockN{Saikrishna Sripada\IEEEauthorrefmark{1},
Y. Raghu Reddy\IEEEauthorrefmark{2}, and
Shivam Khandelwal\IEEEauthorrefmark{1}}

International Institute of Information Technology, Hyderabad (IIIT-H), India \\

\IEEEauthorblockA{\IEEEauthorrefmark{1}Email:
\{saikrishna.sripada,shivam.khandelwal\}@research.iiit.ac.in}
%saikrishna.sripada@research.iiit.ac.in, shivam.khandelwal@research.iiit.ac.in}

\IEEEauthorblockA{\IEEEauthorrefmark{2}Email: raghu.reddy@iiit.ac.in}}%\vspace{-2ex}}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract

%===============================
\begin{abstract}

Gamification has the potential to enhance motivation and help re-invigorate developers to perform repetitive activities like code reviews, change management, knowledge management, issue tracking, etc. Qualitative analysis on the impact of gamification on activities like Collaboration, software implementation, software verification and testing, project management control exist. In this paper we perform Quantitative analysis on the impact of gamification on code review process. We instantiate a gamified version of the code review process from our extensible architectural framework that supports gamification of various software engineering activities. We performed a study consisting of nearly 180+ participants to measure the usefulness of comments given by them during code review process in a gamified versus non-gamified environment. We use text classification algorithms to evaluate the usefulness of code review data. Additionally, measure the impact of gamification by differentiating the code smells and bugs identified in a gamified and non-gamified environment.
%Gamification serves as a method to enhance motivation and help re-iterate boring tasks. Existing studies present the qualitative analysis on the impact of gamification. In this paper we perform Quantitative analysis on the impact of gamification considering code review process as a gamified instance. We have come up with an architectural framework to assist in building a gamified application for code review process. We present the results of Quantitative analysis performed by using the code smells and bugs identified by developers and measuring the usefulness of comments given by them during code review process in a gamified versus non-gamified environment.

%A model is build to make data-driven predictions and identify the usefulness of code review data. Text classification algorithms are used to evaluate the usefulness of code review data. We extracted data from stack exchance Q \& A platfrom to train our model. The final goal of this effort is to identify the classification algorithm suitable for predicting the usefulness of code review data and analyse the impact of Gamification code review process.
\end{abstract}

%================================

\begin{IEEEkeywords}

{

 Gamification, Code Reviews, Text Analysis. %\vspace{-3ex}

}

\end{IEEEkeywords}

%===================================


% done : /home/ssaduser/categorize.csv
% (it is much small though.., only first 100-120 comments are labelled as 0/1, because later i thought it might not be needed, we have got huge data from code review..
% )
% no keywords


% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
%\IEEEpeerreviewmaketitle

%=============================

\section{Introduction}

Gamifying\cite{Deterding2011} \cite{Sebastian2011} a task that is usually considered boring or time taking is seen as an effective way in helping users to performing the task better. Gamification is applied in various domains \cite{Rojas2014}, \cite{Herzig2012}, \cite{Uskov2014}, \cite{Pedreira2015}, \cite{Villagrasa2014} and is seen to be useful in performing the boring tasks with better output. Gamification impact has been studied in academic setup also \cite{Hakulinen2014} \cite{Hakulinen2013}and these studies show the postivite impact of gamification. Gamification of software engineering tasks \cite{Singer2012}, \cite{Biegel2014}, \cite{Tillmann2013}, \cite{Prause2015}, \cite{Phillip2011} often helped developers to perform tasks better. Gamification is also seen as a new method to improve the adoption of a new software engineering method using social software \cite{Schneider2012}. A recent study \cite{Marder2015} has also shown that the effectiveness and adequacy is context-oriented.  All these studies supporting gamification are performing qualitative analysis to support their hypothesis i.e gamification is usefull and advantageous. 

Most of these current studies on gamification deal either with usability of the tool determined by surveys or the involvement of users as a metric to evaluate the usefulness of gamification. Both these methods involves humans. There are quantitative studies to measure the attributes of software engineering practices \cite{Bosu2015}. But the impact of gamification is not being measured quantitatively to evaluate the usefulness of gamification on Software engineering practices. We try to answer this question and find out whether gamification has any impact semantically and adds value or is it just making developers repeat the same boring task again and again just for the numbers to improve. 

To perform this study, we employed five tools to gather the code review comments in both the categories (2 non-gamified and 3 gamified). Supporting arguments on choice of tools we selected is described further in the paper. Out of these five tools which we employed, one tool is built using the framework, which we have developed to suite the needs of gamifying software engineering activities and comprises of the game design elements\cite{Adams2009} suitable for code review activity. The ease of extraction of code review data has a huge impact in selection of the tools we employed. For the empirical study to be feasible, complete code review data involving code review comments, changeset details, date and time of comments, developer details and reviewer details are essential. We extracted this data from all the tools and exported them in .csv format. 

%We train a model to do data-driven predictions and apply this model to predict the usefulness of code review comments given by using the gamified/non-gamified tools.
%The data we extracted is to be cleaned and converted into features, to be fed to a model which will predict the usefulness of comments. For this, we pre-processed the data by following the standard procedure to clean data (involving removing white-space, punctuation, and numbers, converting all words to lower case), followed by removing common English stop-words\cite{link here?}. Then we apply stemming\cite{porter stemming} to reduce the inflected words to their word stem. Word stem is a word without suffixes/prefixes/derivational morphemes (e.g. the derived verbs black-en or standard-ize) etcerta.. This pre-processed data is then served to feature extraction algorithms. As a well known fact, there is no free lunch in finding a better feature extraction algorithm, hence we analysed our study using  TF-IDF, NGram and word2vec algorithms. Our results suggest that word2vec indicates similarity between words for natural language involved in code reviews will generate features when applied on modelling algorithms like SVM/KNN/NB produces better predictions. (Results to show the best combination between TF-IDF|NGRAM|Word2vec and NB|SVM|KNN.

The model we built to predict usefulness has been fed with training data from stackexchange's code review data and the code review comments which we manually classified into useful and not useful based on 1,2,3,4,5 attributes.

%The results can also talk on why Kernel ridge regression or Stochastic Gradient Descent or any classification algorithm (even svm|NB|KNN) may not produce better prediction for SE data.

We imply the model we built to predict usefulness on code review data that is collected on gamified applications and non-gamified applications and visualize the usefulness of gamification quantitatively.

\subsection{Research questions}
\begin{itemize}
\item Does gamification help in performing code review activity with respect to :
    \begin{enumerate}
    \item Improving the usefulness of code review comments given ?
    \item Identifying bugs and detecting code smells in code written by developers?
    \end{enumerate}
\item What are the game design elements required to gamify Code Review process ?
\end{itemize}

%\item Does gamification help in improving user engagement and motivation while performing SE activity(Code review)?
%\item Identify the co-relation between Code Reviews and gamification. Does gamification help?
%\item Found out Game design elements necessary and useful for code review process.
%\item Identify the percentage of usefulness of code review comments. Does gamification helps improve the usefulness of code review comments given?
%\item Find out the best suited text mining and classification algorithms for SE text data.
%\item Use code smells within that code and predict the types of bugs in the code written.
%-- code smells from last year, and bugs in the same, apply labels for training data. Similarly, code smells marked in this year comments. and use it as test data.

%\item present the evaluation of architectural framework built.


%===========
\section{Experimental setup}

%+++++++++++
\subsection{Study Design}
The study has two main phases. During the first phase, background work related to generation of code review data from both gamified and non-gamified environments has been planned and executed. During the second phase, usefulness of code review data is measured quantitatively to establish the impact of gamification on code review process.

\subsubsection{Phase I}
This study was carried out as part of SSAD course offered in IIIT-Hyderabad\cite{Sripada2015}. All students (approximately 183) were taught code review processes. Initially, all these students were given programming assignment which consisited of programming their own Donkey Kong game using oops concepts.
Now, in second phase, all these students were divided into 5 groups each consisting of approx 35 students, and were asked to used one of the code review tool as mentioned in above section.
  
  
Training data from stack exchange platform's code review website and test data from the code review tools deployed by us for this work and code review data of android project are extracted. 


\subsubsection{Gamified Vs non Gamified environments}

\item Why did we choose these three tools.?

%========
\begin{table*}[ht]
\centering
\caption{Comparision and analysis of code review tools choosen for the experiment.}
\begin{tabular}{|c|p{2.5cm}|c||c|c|c|c|c|c|c|} \hline
\multicolumn{2}{|c|}{Tool} & Gamified(Y/N) & Likes & Activity Stream & Points & Leaderboard	& Badges & User Profile & Notifications  \\ \hline \hline
 \setcounter{magicrownumbers}{0}\rownumber & Bitbucket code review platform & No &  & Yes & & & & Yes & Yes \\ 
 \rownumber & Phabricator & No &  & Yes & & & & Yes & Yes \\
 \rownumber & Github commemt counter & Yes &  & & Yes & Yes & & & \\
 \rownumber & codebrag & Yes & Yes & & & & & & Yes \\
 \rownumber & gamified instance generated using the proposed framework & Yes & Yes & Yes & Yes & Yes & Yes & Yes &  \\
 \hline
\end{tabular}
\label{tab:gamedesignelements}
\end{table*}
%=========


%+++++++++++
\subsubsubsection{Architectural framework for building gamified applications.}

%=====
\begin{figure*}[ht]
%\hspace{-6ex}
\centering
\includegraphics[width=1\textwidth]{images/arch.png}
\caption{Architecture of the gamified application.}
\label{fig:architecture}
\end{figure*}
%====

\subsection{Text classification and prediction process}
%=====
\begin{figure}[ht]
%\hspace{-3ex}
\centering
\includegraphics[width=0.5\textwidth]{images/TextClassification_and_Prediction.png}
\caption{Text Classification and prediction Process.}
\label{fig:activity}
\end{figure}
%====
%+++++++++++
\subsection{Dataset Generation}
\subsubsection{Training data set from stackexchange.}
The training data set served for our model is extracted from code review product\footnote{http://codereview.stackexchange.com/} of stack exchange platform. While determining the useful set of review comments from the stack exchange platform, we followed the below pattern :
Firstly, we chose "python" and its related tags \footnote{http://codereview.stackexchange.com/tags} that are suitable for our code review data. We have applied "python" "python2.7", "algorithms", "datastructures", "begineer", "oop", "strings", "parsing" while extracting comments from code review platform of stackexchange. \newline During the second step, We chose the questions in the order of those that have higher upvotes to lesser upvotes. \newline Third, The most upvoted and zero upvotses/downvoted code review comments for these selected questions are extracted. \newline Fourth, We have manually assigned labels to perform feature extraction on the training data.

\subsubsection{Test data set extracted from the code review tools used in the experiment.}
The test data for our experiments is the code review data collected from $183$ students who performed code review using the five tools. The number of comments students has given while performing code review is shown in Table \ref{tab:commentscount}.

%+++++++++++
\subsection{Preprocessing}
The data set from various stackexchange platforms is extracted and made available publicly \footnote{https://archive.org/download/stackexchange}. We have awarded normalized scores to answers that 

\subsubsection{Attributes for code review comments in stack exchange} \label{Attributes}

%answer_vote_count': 10,
%is_accepted': 12, 
%thread_has_accepted': -3, 
%question_vote_count': 8, 
%reviewer_repo': 7,
%\# 'ratio_ans_and_best_ans': 1,
%avg_repo_question_people_involved': 6,
%avg_repo_answer_people_involved': 5,
%people_involved_question': 4, # Total number of users involved in the thread.
%people_involved_answer': 3, #people who have commented on the answer.
%ans_count': 1

\begin{itemize}
\item Is the answer accepted or not.
\item \# people involved in an upvoted answer and \# people involved in the code review thread.
\item \# of comments within each Answer.
\item Upvotes.
\item Reputation of reviewer.
\end{itemize}

%+++++++++++
\subsubsection{Normalization and Feature Extraction}

All the attributes mentioned in \ref{Attributes} are assigned a normalized score for each of the code review thread. The Attributes  are initially standardised using a Minimum-Maximum scale from zero to one. This is performed using scikit-learn "preprocessing" module. This scaled data is assigned weight according to the following rules :

\begin{enumerate}
\item affa
\end{enumerate}
The data is then obtained as a numerical vector and is normalised following the above rules, where a higher score indicates the highest usefulness of comments. The Highest and lowest weights obtained are $highest$ and $lowest$

A set of stop words that are used to enhance the features of code review data is inherited from \footnote{ttps://github.com/Alir3z4/stop-words} 

%+++++++++++
\subsubsection{Characteristics of Usefulness of comments}
We have collected $1600$ comments from stack exchange's code review platform under the python tag. All these comments are related to python code review questions. The top 50 stemmed terms in most useful category of accepted comments and unaccepted comments are shown in Table \ref{tab:Top50}.

%=====
\begin{table}[ht]
\centering
\caption{ Most useful $50$ list of words after stemming is performed on words from code review comments}
\begin{tabular}{|cccc|} \hline
\multicolumn{4}{|c|} {Top Terms from most upvoted comments }  \\ \hline
function & python & list & make \\
class  & variable  & call &  method \\
number  & line &  time &  loop \\
name &  write &  case  & string \\ 
return  & file  & thing  & test \\ 
check &  good  & comment  & work \\ 
program  & implement  & suggest  & read \\ 
problem &  object  & point &  iter \\ 
run &  avoid &  set &  change \\
find &  data &  create &  result \\ 
style &  note &  generate  & import \\
value  & start &  remove &  user \\
oper & & & \\
\hline

%\multicolumn{4}{|c|} {Top Terms from most downvoted comments} \\ \hline
%i & j & k & l \\
%m & n & o & p \\
%\hline
\end{tabular}
\label{tab:Top50}
\end{table}
%=====

%+++++++++++++++++++
\subsection{Predictive Model Generation}



%===========================

\section{Emperical Analysis}

%++++++++++++++
\subsection{Results}

%========
\begin{table}[ht]
\centering
\caption{Count of code review comments given by students.}
\begin{tabular}{|c|p{2.5cm}|p{1.5cm}|p{1.5cm}|c|} \hline
\multicolumn{2}{|c|}{Tool} & \# students involved in code review & Number of comments & Average  \\ \hline \hline
 \setcounter{magicrownumbers}{0}\rownumber & Bitbucket code review platform & $36$ & $2935$ & $81$ \\ 
 \rownumber & Phabricator & $34$ &  $1979$  & $58$ \\
 \rownumber & Github commemt counter & $37$ & $1510$ & $41$ \\
 \rownumber & codebrag & $37$ & $2778$ & $75$ \\
 \rownumber & gamified instance generated using the proposed framework & $39$ & $2950$ & $75$ \\
 \hline
\end{tabular}
\label{tab:commentscount}
\end{table}
%=========

%Is there any reason y the students are not divided into equal number??
Other useful stats

Total
$12152$

Overall Avg Comments (considering 180)
$67.5$

%====
\begin{table}[h]
\centering
\caption{Most common game design elements required.}
\begin{tabular}{|ccc|} \hline
 \multicolumn{3}{|c|}{Most common game designed elements for Code Review process}  \\ \hline
 a & a & a \\
 a & a & a \\
 \hline
\end{tabular}
\label{tab:gamede}
\end{table}
%====

%====
\begin{table}[h]
\centering
\caption{Correlation between bug types and code smells within code.}
\begin{tabular}{|c|c|c|c|c|} \hline
 Group Number & \# student & Tool & \# Bug & \# code smells \\ \hline
 & & & &\\
 \hline
\end{tabular}
\label{tab:bugsandsmells}
\end{table}
%====

%====
\begin{table}[h]
\centering
\caption{Analysis of code smells and bugs found during code review process.}
\begin{tabular}{|p{3.5cm}|p{2cm}|p{2cm}|} \hline
Aspect & Gamified environment & Non Gamified environment \\ \hline
Average time spent by students & & \\
Number of Bugs identified  & & \\
Number of code smells &  & \\
\hline
\end{tabular}
\label{tab:bugsandsmells}
\end{table}
%====

%++++++++++++++
\subsection{Results from data generated by tools}
These are some parameters which would be evaluated tool wise.
\begin{itemize}
\item Analyse the comments (usefulness)
\item 
\item Access logs (if possible)
\end{itemize}


%++++++++++++++
\subsection{Visualization of usefulness of comments with and without Gamification.}
The usefulness of comments is calcluates using "precision", "recall", "accuracy" and "Fscore" for each method.

%===================================================
\begin{table}[h]
\centering
\caption{Results}
\begin{tabular}{|c|p{2cm}|c|c|c|c|}\hline
\multicolumn{2}{|c|}{Classification Algorithm} & Precision & Recall & Accuracy & FScore  \\ \hline \hline
 \setcounter{magicrownumbers}{0}\rownumber & NB & & & &  \\ 
 \rownumber & SVM & & & & \\
 \rownumber & KNN & & & & \\
 \rownumber & Decision Tree & & & & \\
 \rownumber & Random Forest & & & & \\
 \hline
\end{tabular}
\label{tab:results}
\end{table}

%========================
\section{Discussion}
A qualitative study still supports gamification which can be understood by the count of code review comments given or by the amount of time spent on reviews or by the number of times a developer has come back to review the code. While the count of bugs and code smells identified remains same in both gamified and non gamified setup, the quantitative analysis also suggests that, the usefulness of comments remains same and doesnot enhance anything in a gamified environment. Hence more and more studies are required to be performed on quantitative impact of gamification. This will give an ideal measure of usefulness of gamification and justifies the amount spent on gamification.

\subsection{Does success of gamified application depend on the motivation behind gamification??}
The contribution from this work will add support to the view of gamification is only seen as useful but the impact on actual process is relatively negligible when compared semantically and the value add it is bringing to software engineering field. Tasks such as project management, work hour management of a developer, or making the developer more disciplined with his personal management can be achieved by introducing gamification. But the application of gamification on product development might not have real meaningful impact. Hence we bring in the point that, gamification is more usefull when applied over process orientation but not into product development to improve quality/ performance of a system/ or to come up with a better design for the system.

This study of ours also give rise to the discussion on the motivation for gamifying an application. If we consider activities like code reviews in industry, those who review the code and present their comments are usually domain experts. Hence the amount of spam with in those comments will be less, but studies which analyse code review platforms \cite{openstack results presented during the semister presentation} suggest that functionality to be delivered in a product will be pushed over release cycles due to the code reviews still pending from quality gate keepers(reviewers) who usually will be the owner of the module in a project. But when code review process is implemented in academic level, where the reviewers are still student and their expertise with respect to domain, technologies and frameworks is limited might rush through to present the comments but those might not be as useful. So gamifying the code review process in both these cases involves a different set of game design elements that will be suitable to improve usefulness Versus those which are suitable to reduce the review time for each review.

Hence we feel that, depending on studies like these, identifying the motivation of gamifying an application and deciding on the right game design elements are a key to the success of gamification.

\subsection{Why did we choose to use data from stack exchange's code review platform as training data? }
The Reasons/Benifits of using stack exchange are as follows :  1) In github in-line code review tasks, the code will be reviewed for which a pull request is created. Once the comments are answered and an updated code is pushed into the repository, The comments given during an earlier pull request and the code attached with the pull request will bot be in syn. Hence The code review comments extracted, might not be valid for the existing code after a point of time. Whereas in stack exchange data, the comments and code that is under review will remain in-tact. 2)  Other code review tools like gerrit and phabricator will also store the code that is under review and sync the code with the comments. But identifying the usefulness of code review comments cannot be done since there are no metrics to identify the same. In the case of stack exchange data, the question posted for the code to be under review and the code review comments given for the question will have upvotes and downvotes from the community which includes developers who have expertise in the related technology. This gives us the oppurtunity to identify the usefulness of code review comments. 3) The third reason being that stack exchange websites already include game design elements such as upvotes and points. User profile, badges are also involved and the reward system of stack exchange is robust in avoiding spam comments. Since stack exchange websites are already gamified to certain extent, this brings in similarity between the review data between stack exchange(Training data) and the review data collected from the tools(Test data) involved in our experiment. 

\subsection{What feature extraction and classification algorithms suite code review data?}
Although word2vec is considered as better for feature extraction for text, In our case, Term frequency has worked better. After performing feature extraction with TF-IDF, Ngram and word2vec and applying these three vectors on all the four chosen classification algorithms, we found out that vectors generated using TF-Idf when served to Decision tree classification algorithm has yield us the best results. The reason being Low "false negatives" and higher "Accuracy".

The model built by us can be used for any software engineering activity. The vocabulory we used is generated from code reivew activity and is specific to this activity. Hence to use our model for any other software engineering activity, the vocubulory has to be changed accordingly.

We also hypothesize that, vocabulory specific to a particular software engineering activity is more suitable for usage while training a model for specific activity rather than using a common set of words for all the software engineering activities.

\subsection{Are the comments given by students relatively less useful ?}
The comments given by students are seen as lesser useful compared to the comments given by developers of android project. We observe this to be a natural phenomena since the students who performed the code reviews in our experiments are doing the code review for the first time and are relatively new to the coding standards, design patterns and the devlopers of android project have good project developer experience as well as those developers are usually experienced in the techonology they use. 


%=========================
\section{Related work.}

%\subsection{Code Reviews.}

Code reviews involve developers who are expert in the domain and technology to manually walk through the code line by line and give comments which can identify functionality defects, identify variations in coding style followed, performance enhancements, suggestion of better patterns, identify code smells, suggestions to re-factor code etc. while Peer code reviews act as an effective strategy to catch bugs in early stages of product development \cite{Bacchelli2013}, as a medium for improving Communication Skill Development and Active Learning\cite{Anewalt2005}, to assess coding standards \cite{Li2006} and improve software quality\cite{McIntosh2014} it is often seen as a boring task. Gamification of code review process has been under study since sometime now\cite{emerson murphy hill paper}, \cite{Prause2015}. 

%\subsection{Gamification.}

Gamification is the use of {\it Game Design elements, Game Mechanics} and {\it Game Thinking} in non-gaming contexts to improve the userâ€™s engagement, motivation, and performance. Gamification impact has been studied in academic setup also \cite{Hakulinen2014}, \cite{Hakulinen2013} and these studies show the postivite impact of gamification. SAP work needs to be quoted. Regarding the raise in users.

Game design elements are tools and techniques required to gamify an application \cite{Adams2009}. They play an important role in determining the success of a gamified application in any domain. Gamification design process is the process of designing and applying game design elements to create the rules and modes of operation for a gamified application. For supporting gamification of software engineering activities, any platform supporting the gamification of applications should support invocation and usage of any/all the game design elements required to gamify those activities. Pedreira et al. \cite{Pedreira2015} found out that \textit{"points"} \textit{"Badges"} are the most commonly used game design elements. Communities and social learning environments improve the learning activity\cite{Vassileva2008},\cite{Begel2013} \cite{Etienne1998}. Hence while gamifying applications to  performing software engineering activities, it is important to consider these game design elements which enable a social platform and assist in forming communities.

%subsection{Text classification and prediciton is SE.}
Training data and test data. Text classification. Usefulness of comments.\cite{Bosu2015} \cite{Siersdorfer2010}.
Text classification algorithms usage and applications in field of software engineering.Fault localization : \cite{Roychowdhury2011} , Bug prediction : \cite{Jiang2007} in sentiment analysis \cite{Sharma2012}, spam filtering in emails \cite{Panigrahi2012}

%\subsection{stackexchange}
 Stack exchange\footnote{http://stackexchange.com/} is a network of Question and Answer websites which provides support for various fields. Code Review\footnote{http://codereview.stackexchange.com/} is a website for code reviews in software engineering field. Their gamification element "Upvote" plays a crutial role in identifying the correct answer. Earlier studies \cite{Posnett2012} show that the reputation of an experienced user does not add any additional advantage in Stack Exchange platform. A study \cite{Allamanis2013} also shows that stack exchange questions are not specific to application domain specific but are about code and technologies. Stack  overflow has inbuilt game design elements\cite{Marder2015} and this make the training data extracted from stack exchange website more suitable for our study. To our best knowledge this is the first study to use "code review data" from stack exchange website.


%\subsection{Code smells and bug types.}

%\subsection{Text Mining.}

%Software engineering activities while designing a product involves various activities right from requirement gathering to maintenance of product. As a whole the process that is followed while and after building the product will play a definite role in the cost of the product. Bug detection for example, bugs caught during the testing  phase or even early will reduce the maintenance cost of a product many fold while compared to the same bug caught after release of product. \cite{needed}. This process of bug detection can be further moved into early development phase by performing code inspections and code reviews. 



%\subsection{Metrics to evaluate architectural framework.}




%\subsection{Text Analysis.}
%\subsubsection{Feature extraction and classification algorithms.}

%===========================
\section{Threats to validity.}

Our Study is restricted to one software engineering activity i.e code reviews. Although we have used four different text classification algorithms and chose the best among those four, there are other classification algorithms suitable for text which might yield better results. Another restriction here is, since we have text in a well organised format, we have restricted ourselves to supervised learning algorithms only. 

While training the model, we have considered only two sets of data which is extracted from projects on only python and java technologies. While the process of code review is language independent, The vocabulary we build might not be sufficient to bring in the usefulness of projects belonging to c++, haskel or any other technology. Hence while using the model built by us to measure usefulness of code reviews which are generic or specific to any other language other than python and java, it is suggested to train the model using atleast $100$ comments from the data set by classifying them into useful and non-useful.


%=========================

\section{Conclusion and Furute Work.}

As a future work, We target to classify the software engineering tasks that will be suitable for gamification and those which will not have a meaningful impact of gamification.

%==========================
%\begin{thebibliography}{1}
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,quasoq}
\end{document}


